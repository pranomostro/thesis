% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Approach}\label{chapter:approach}

\section{Replicating ''Scalable Approaches for Test Suite Reduction''}

This paper attempts to replicate the findings in
\cite{cruciani2019scalable}, using different test data and adding a
further algorithm as a baseline. \cite{cruciani2019scalable} builds on
\cite{miranda2018fast} (which introduced the FAST family), and adds two
new algorithms to the FAST family.

We use the code from the original paper, published online at
\url{https://github.com/ICSE19-FAST/FAST-R}, with some slight
modifications to fix faults discovered in the original code.

\section{Implemented Algorithms}

\cite{cruciani2019scalable} compares seven different methods of
test-suite reduction. Four of those (\textbf{FAST++}, \textbf{FAST-all},
\textbf{FAST-CS} and \textbf{FAST-pw}) are in the FAST family
(first introduced in \cite{miranda2018fast}), which uses clustering
techniques to find representative test cases. Two other algorithms are
taken from the similarly clustering-based ART family, first developed
in \cite{chen2010adaptive}. The last reduction method examined in
\cite{cruciani2019scalable} is the Greedy Additional (\textbf{GA})
algorithm developed in \cite{rothermel2001prioritizing}, which is included
because "for its simplicity and effectiveness [it] is often considered
as a baseline."

This paper also includes a random selection algorithm as a baseline,
as recommended by \cite{khan2018systematic}.

\subsection{FAST}

The FAST family is a clustering-based family of algorithms for test
suite reduction (\cite{miranda2018fast}, \cite{cruciani2019scalable}).

The FAST family is a collection of 4 clustering based test suite
reduction algorithms that work both with adequate and inadequate test
suite reduction problems, especially for large test suites.

\subsubsection{FAST++}

% CANDO: Say that it uses python's vectorizer.fit_transform for this?

The FAST++ algorithm starts with a preparation phase: the tests from $T$
are transformed into points in a vector space by treating each token
(e.g. character) of the test case as a dimension, with the value of that
dimension being the value of the token at the position (e.g. the value
of the nth character), with the components being "weighted according to
[a] term-frequency scheme, i.e., the weights are equal to the frequency
of the corresponding terms" (\cite{cruciani2019scalable}).

Since dealing with high-dimensional vector spaces is computationally
costly (e.g. when computing the Euclidean distance), the algorithm
performs a random projection into a lower-dimensional vector-space that
nonetheless still mostly preserves the pairwise distances of the vectors
(in this case a sparse random projection (\cite{achlioptas2003database})).

The second phase of FAST++ is executing the k-means++ algorithm \cite{arthur2006k}
on the resulting vectors, with k in the inadequate case being the
budget of the reduction, and in the adequate case being a variable
that is incremented until the requirements are met. The k-means
algorithm is a clustering algorithm that finds k clusters of vectors in a
high-dimensional space, i.e. minimizing the distance between points within
a cluster, by iteratively assigning points to the nearest mean (distance
being measured in squared Euclidean distances) and recalculating means
until a fixed point is reached. The k-means++ algorithm only differs
from k-means in the method for choosing initial centers of clusters:
While k-means selects values at random, k-means++ selects the initial by
computing the center of all points, and then selecting k other centers by
sampling points using a distribution proportional to the distance of the
points to the global mean. This both increases speed %CANDO: citation?
and gives guarantees that the solution is $\mathcal{O}(\log k)$ competitive
to the optimal solution. %CANDO: [citation needed]

After computing the k clusters, FAST++ returns the vectors closest to
the centers of the k clusters (i.e. the tests most representatitve for
those clusters).

\subsubsection{FAST-CS}

The FAST-CS algorithm has the same preparation phase as FAST++: test
cases are vectorized, and the resulting vectors are projected into a
lower-dimensional vectorspace.

The second phase is different, as it attempts to cluster the set of
points by finding a coreset, a set of points that approximate the shape
of the set of vectors.

%CANDO: find a source for the first description of coreset

This is achieved by importance sampling: "All points have nonzero
probability of being sampled, but points that are far from the center
of the dataset (potentially good centers for clustering) are sampled
with higher probability."

The metric used for importance sampling is as follows:

$$
	Q(t) \leftarrow \frac{1}{2|T|}+\frac{d(P(t),\mu)^2}{\sum_{t' \in P}d(P(t'),\mu)^2}
$$

where $\mu$ is the mean of the whole dataset, $T$ is the test suite,
$d$ is a distance metric (in this case the Euclidean metric), and $P$
is the random projection of the test suite.

The points are sampled without replacement until either the coverage is
adequate or the budget has been reached.

\subsubsection{FAST-pw}

FAST-pw was first introduced in \cite{miranda2018fast} as a test suite
priorization algorithm. It attempts to maximize the Jaccard distance
between early test cases (the Jaccard distance between two sets $A$
and $B$ being $JD(A, B)=1-\frac{|A \cap B|}{|A \cup B|}$).

The algorithm uses three different concepts: \textit{shingling},
\textit{minhashes} (and \textit{minhash signatures}), and
\textit{locality-sensitive hashing}.

\textit{Shingling} is used as a method for transforming a test case into
a set, and is therefore only used in the case where coverage information
is not available. Given a string of $S$, a k-shingle is the set of all
substrings of $S$ of length k. For example, the 7-shingle of "bananas"
is just the set $\{"bananas"\}$, the 5-shingle of "bananas" is $\{"banan",
"anana", "nanas"\}$. \cite{miranda2018fast} states that "if two documents
are similar they will have many shingles in common".

However, as \cite{miranda2018fast} observes, sets of shingles (or of
coverage information) can be very large.

\textit{Minhashing} is a method for deriving compact representations of
sets. This is achieved by creating a list of hash functions $H=\{h_1,
\dots h_k\}$, and for each test case $T$ (a set of shingles or of
coverage information) calculating the list $[\hbox{arg min}_{t \in T}
h_1(t), \dots \hbox{arg min}_{t \in T} h_k(t)]$ (which is called the
signature of the set). Less formally, the minhash at position i is the
minimal value of the hash function for any element of the set.

The Jaccard distance of two sets can be estimated by calculating the
number of positions on which the minhash signatures $s_1$, $s_2$ of the
two sets agree:

$$\hbox{EstimateJD}(s_1,s_2)=1-\frac{|\{i|i\in 1..k, s_1(i)=s_2(i)\}|}{|s_1|}$$

\textit{Locality-sensitive hashing} is a further technique to make
runtimes shorter. Let $S=\{s_1, \dots s_n\}$ be a set of minhash
signatures of all tests. Now a matrix $M \in \mathbb{R}^{n \times k}$
is created, by using the minhash signatures as columns. The rows of this
matrix are now divided into bands of length $r$ (in the example 2):

% CANDO: add matrix stuff, LSHs, lines between buckets, row indicators
% See sheet notes

\begin{figure}[!ht]
\[
\begin{blockarray}{cccccccccc}
	& s_1 & s_2 & s_3 & s_4 & s_5 & s_6 & s_7 & s_8 & \\
	\begin{block}{c[cccccccc]c}
		r_1 & 1 & 16 & 9 & 5 & 5 & 1 & 2 & 6 & b_1 \bigstrut[t] \\
		r_2 & 2 & 19 & 16 & 18 & 0 & 9 & 5 & 5 & \\
		\\\hhline{~--------~}
		& 8 & 10 & 9 & 14 & 1 & 3 & 11 & 2 & lsh_1 \\
		\\\hhline{~========~}
		r_3 & 10 & 8 & 4 & 11 & 18 & 9 & 16 & 7 & b_2 \\
		r_4 & 12 & 14 & 3 & 5 & 8 & 3 & 1 & 16 & \bigstrut[b]\\
		\\\hhline{~--------~}
		& 3 & 5 & 1 & 6 & 17 & 13 & 12 & 7 & lsh_2 \\
		\\\hhline{~========~}
		r_5 & 10 & 11 & 5 & 18 & 14 & 6 & 14 & 7 & b_3 \\
		r_6 & 18 & 5 & 2 & 13 & 7 & 19 & 9 & 11 & \\
		\\\hhline{~--------~}
		& 7 & 6 & 6 & 13 & 6 & 14 & 12 & 16 & lsh_3 \\
		\\\hhline{~========~} \\
	\end{block}
\end{blockarray}\vspace*{-1.25\baselineskip}
\]
\caption{Locality-Sensitive Signature Hash Matrix}\label{tab:lsh_matrix}
\end{figure}

$b_n$ is the nth band, $r_n$ is the nth row, and $s_n$ is the nth
signature.

The columns within the bands are hashed into lsh values. If two lsh
values correspond (e.g. in the case where the columns with values 11
and 5 are both hashed into 6), they are put into the same candidate set.

When the hash of the values in different columns of two bands is the
same (the two minhashes are in the same bucket), the two test cases the
minhashes belong to are added to the confusingly named candidate set
(tests in the candidate set are excluded from selection). The candidate
set contains tests that have a Jaccard distance over a threshold s,
where $s \approx 1-(r/n)^{1/r}$.

The algorithm follows different steps:

\begin{itemize}
	\item[] 1. Calculate the minhashes
	\item[] 2. Calculate the candidate set $C_s$
	\item[] 3. Select a test from the set of remaining tests $T \backslash C_s$
	\item[] 4. Remove that test from the remaining tests $T$ and add it to the set of selected tests $S$
	\item[] 5. If the condition is not fulfilled (budget not fulfilled in the inadequate case, coverage not achieved in the adequate case), go to step 2
	\item[] 6. Return $S$
\end{itemize}

%CANDO: formulate less confusingly?

The function that selects the test case from the remaining test cases
maximizes the Jaccard distance of the returned test and the so far
selected test cases $S$ (where $M$ is a function that produces minhashes, and Estimate:

$$ \hbox{arg max}_{c \in T \backslash C_s} \hbox{EstimateJD}(M(S), M(c))$$

\subsubsection{FAST-all}

FAST-all is very similar to FAST-pw. The only difference lies in the
selection function: While FAST-pw chooses based on Jaccard distance,
FAST-all selects all of $T \backslash C_s$, simply choosing all
non-candidate tests. This is a special case for the originally described
algorithm, which allowed to select a random sample from the non-candidate
tests with a given size ($one$, $log$, $sqrt$, $all$).

\subsection{Random Selection}

\cite{khan2018systematic} gives 12 recommendations on how to execute and
evaluate test suite reduction experiments. One of them is to include a
baseline algorithm to compare the other algorithms to, such as random
selection.

In the inadequate case, random selection simply chooses $B$ random tests
from the test set (Algorithm \autoref{code:rsbudget}).

%TODO: fix the references for the pseudocode

\begin{algorithm}
\caption[Random selection, budget]{Random selection, budget}\label{code:rsbudget}
\begin{algorithmic}
\Function{rsbudget}{$TS, B$}
	\State \textbf{return} $\hbox{randomsubset(TS, B)}$
\EndFunction
\end{algorithmic}
\end{algorithm}

In the adquate case, a random element is removed from the original test
suite and added to the selected tests until the set of selected tests
fulfills all requirements (in this case, covers all coverable lines). See
Algorithm \autoref{code:rsadequate}.

\begin{algorithm}
\caption[Random selection, adequate]{Random selection, adequate}\label{code:rsadequate}
\begin{algorithmic}
\Function{rsadequate}{$TS, cov, B$}
	\State $RS \gets \emptyset$
	\State $allcov \gets \bigcup_{t \in TS} \hbox{cov}(t)$
	\While{$\bigcup_{t \in RS} \hbox{cov}(t) \neq allcov$}
		\State $s \gets selectrandom(TS)$
		\State $TS \gets TS \backslash \{s\}$
		\State $RS \gets RS \cup \{s\}$
	\EndWhile
	\State \textbf{return} RS
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Adaptive Random Testing}

The second family of algorithms compared to in this paper is the ART
(Adaptive Random Testing) family introduced in \cite{jiang2009adaptive}
and elaborated on in \cite{chen2010adaptive}. These algorithms were
originally developed for test case priorization (as an alternative
to random selection), but can be trivially converted into test suite
reduction algorithms by applying the priorization procedure and then
either selecting the first $B$ test cases or so many that they fulfill
all requirements.

\subsubsection{ART-D}

The ART-D algorithm was presented in \cite{jiang2009adaptive} (although
there only called ART). It starts with a set of tests $T$ and an empty
sequence of prioritized tests $P$. The algorithm works in two stages:

First, tests are selected from $T$ and added to a candidate set $C$
as long as new tests add new coverage information to $C$ (this will not
result in the joint coverage of tests in $C$ being the joint coverage of
tests in $T$). This results in the size of candidate sets being variable
during exection.

Second, a test $c$ is selected from $C$ and appended to $P$. The
criterion for selecting $c$ is to maximize the distance between all $p
\in P$ and $c$; \cite{jiang2009adaptive} use the Jaccard distance in
coverage and examine different methods for comparing the sequence of
priorizited test cases to the tests in the candidate set (maximizing
the maximum/average/minimum distance between the selected test case and
the already priorized cases), and conclude that maximizing the minimal
distance might be the best strategy.

The selected test case is appended to the sequence of priorizited test
cases and removed from $T$. Unless $T$ is now empty, the first stage is
executed again.

\subsubsection{ART-F}

The ART-F algorithm was introduced in \cite{zhou2012fault}. It
is similar in structure to ART-D, but uses the Manhattan distance
instead of the Jaccard distance, and frequency information instead
of coverage information, as well as having a fixed-size candidate
set. Frequency information is information about how often a part of the
SUT is executed by a test. However, in the presented experiment, since
frequency information is not available, coverage information is used as
a place-holder, where a test covering a statement is treated as the test
executing tha statement with frequency 1. The Manhattan distance of two
sequences of frequency information is calculated as such:

$$\hbox{MD}(u, v)=\sum_{i=1}^{n}|u_i-v_v|$$

The stages in ART-F are the same as in ART-D, and it also returns a
sequence of prioritized test cases.

\subsection{Greedy Algorithm}

The greedy algorithm used is the greedy additional algorithm from
\cite{rothermel2001prioritizing}: In each iteration, it selects the test
case with the highest number of yet uncovered program entities.

\begin{algorithm}
\caption[Greedy selection next test case]{Greedy selection next test case}\label{code:ganext}
\begin{algorithmic}
\Function{ganext}{$RS, TS, cov$}
	\State $\hbox{alreadycov} \gets \bigcup_{t \in RS} \hbox{cov}(t)$
	\State \textbf{return} $\hbox{arg max}_{t \in TS} |cov(t)\backslash \hbox{alreadycov}|$
\EndFunction
\end{algorithmic}
\end{algorithm}

In the adequate case, we select greedily until we achieve complete
coverage:

\begin{algorithm}
\caption[Greedy selection, adequate]{Greedy selection, adequate}\label{code:gaadequate}
\begin{algorithmic}
\Function{gaadequate}{$TS, cov$}
	\State $RS \gets \emptyset$
	\State $allcov \gets \bigcup_{t \in TS} \hbox{cov}(t)$
	\While{$\bigcup_{t \in RS} \hbox{cov}(t) \neq allcov$}:
		\State $s \gets ganext(RS, TS, cov)$
		\State $TS \gets TS \backslash \{s\}$
		\State $RS \gets RS \cup \{s\}$
	\EndWhile
	\State \textbf{return} RS
\EndFunction
\end{algorithmic}
\end{algorithm}

In the budget case, we instead select greedily $B$ times.

\begin{algorithm}
\caption[Greedy selection, budget]{Greedy selection, budget}\label{code:gabudget}
\begin{algorithmic}
\Function{gabudget}{$TS, cov, B$}
	\State $RS \gets \emptyset$
	\State $allcov \gets \bigcup_{t \in TS} \hbox{cov}(t)$
	\For $B$ times
		\State $s \gets ganext(RS, TS, cov)$
		\State $TS \gets TS \backslash \{s\}$
		\State $RS \gets RS \cup \{s\}$
	\EndFor
	\State \textbf{return} RS
\EndFunction
\end{algorithmic}
\end{algorithm}
