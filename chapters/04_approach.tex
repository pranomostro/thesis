% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Approach}\label{chapter:approach}

8 pages

\section{Replicating ''Scalable Approaches for Test Suite Reduction''}

This paper attempts to replicate the findings in
\cite{cruciani2019scalable}, using different test data and adding a
further algorithm as a baseline. \cite{cruciani2019scalable} builds on
\cite{miranda2018fast} (which introduced the FAST family), and adds two
new algorithms to the FAST family.

We use the code from the original paper, published online at
\url{https://github.com/ICSE19-FAST/FAST-R}, with some slight
modifications to fix faults discovered in the original code.

1 page

\section{Implemented Algorithms}

\cite{cruciani2019scalable} compares 7 different methods of
test-suite reduction. 4 of those (\textbf{FAST++}, \textbf{FAST-all},
\textbf{FAST-CS} and \textbf{FAST-pw}) are in the FAST family
(first introduced in \cite{miranda2018fast}), which uses clustering
techniques to find representative test cases. 2 other algorithms are
taken from the similarly clustering-based ART family, first developed
in \cite{chen2010adaptive}. The last reduction method examined in
\cite{cruciani2019scalable} is the Greedy Additional (\textbf{GA})
algorithm developed in \cite{rothermel2001prioritizing}, which is included
because "for its simplicity and effectiveness [it] is often considered
as a baseline."

This paper also includes a random selection algorithm as a baseline,
as recommended by \cite{khan2018systematic}.

7 pages

\subsection{FAST}

The FAST family is a clustering-based family of algorithms for test
suite reduction (\cite{miranda2018fast}, \cite{cruciani2019scalable}).

The FAST family is a collection of 4 clustering based TSR algorithms
that work both with adequate and inadequate TSR problems, especially
for large test suites.

4 pages

\subsubsection{FAST++}

% CANDO: Say that it uses python's vectorizer.fit_transform for this?

The FAST++ algorithm starts with a preparation phase: the tests from $T$
are transformed into points in a vector space by treating each token
(e.g. character) of the test case as a dimension, with the value of that
dimension being the value of the token at the position (e.g. the value
of the nth character), with the components being "weighted according to
[a] term-frequency scheme, i.e., the weights are equal to the frequency
of the corresponding terms" (\cite{cruciani2019scalable}).

Since dealing with high-dimensional vector spaces is computationally
costly (e.g. when computing the Euclidean distance), the algorithm
performs a random projection into a lower-dimensional vector-space that
nonetheless still mostly preserves the pairwise distances of the vectors
(in this case a sparse random projection).
%CANDO: cite Achlioptas and Lindenstrauss here

The second phase of FAST++ is executing the k-means++ algorithm %CANDO: cite Arthur & Vassilvitskii
on the resulting vectors, with k in the inadequate case being the
budget of the reduction, and in the adequate case being a variable
that is incremented until the requirements are met. The k-means
algorithm is a clustering algorithm that finds k clusters of vectors in a
high-dimensional space, i.e. minimizing the distance between points within
a cluster, by iteratively assigning points to the nearest mean (distance
being measured in squared Euclidean distances) and recalculating means
until a fixed point is reached. The k-means++ algorithm only differs
from k-means in the method for choosing initial centers of clusters:
While k-means selects values at random, k-means++ selects the initial by
computing the center of all points, and then selecting k other centers by
sampling points using a distribution proportional to the distance of the
points to the global mean. This both increases speed %CANDO: citation?
and gives guarantees that the solution is $\mathcal{O}(\log k)$ competitive
to the optimal solution. %CANDO: [citation needed]

After computing the k clusters, FAST++ returns the vectors closest to
the centers of the k clusters (i.e. the tests most representatitve for
those clusters).

\subsubsection{FAST-CS}

The FAST-CS algorithm has the same preparation phase as FAST++: test
cases are vectorized, and the resulting vectors are projected into a
lower-dimensional vectorspace.

The second phase is different, as it attempts to cluster the set of
points by finding a coreset, a set of points that approximate the shape
of the set of vectors.

%CANDO: find a source for the first description of coreset

This is achieved by importance sampling: "All points have nonzero
probability of being sampled, but points that are far from the center
of the dataset (potentially good centers for clustering) are sampled
with higher probability."

The metric used for importance sampling is as follows:

$$
	Q(t) \leftarrow \frac{1}{2|T|}+\frac{d(P(t),\mu)^2}{\sum_{t' \in P}d(P(t'),\mu)^2}
$$

where $\mu$ is the mean of the whole dataset, $T$ is the test suite,
$d$ is a distance metric (in this case the Euclidean metric), and $P$
is the random projection of the test suite.

The points are sampled without replacement until either the coverage is
adequate or the budget has been reached.

\subsubsection{FAST-pw}

FAST-pw was first introduced in \cite{miranda2018fast} as a test suite
priorization algorithm. It attempts to maximize the Jaccard distance
between early test cases (the Jaccard distance between two sets $A$
and $B$ being $JD(A, B)=1-\frac{|A \cap B|}{|A \cup B|}$).

The algorithm uses three different concepts: \textit{shingling},
\textit{minhashes} (and \textit{minhash signatures}), and
\textit{locality-sensitive hashing}.

\textit{Shingling} is used as a method for transforming a test case into
a set, and is therefore only used in the case where coverage information
is not available. Given a string of $S$, a k-shingle is the set of all
substrings of $S$ of length k. For example, the 7-shingle of "bananas"
is just the set $\{"bananas"\}$, the 5-shingle of "bananas" is $\{"banan",
"anana", "nanas"\}$. \cite{miranda2018fast} states that "if two documents
are similar they will have many shingles in common".

However, as \cite{miranda2018fast} observes, sets of shingles (or of
coverage information) can be very large.

\textit{Minhashing} is a method for deriving compact representations of
sets. This is achieved by creating a list of hash functions $H=\{h_1,
\dots h_k\}$, and for each test case $T$ (a set of shingles or of
coverage information) calculating the list $[\hbox{arg min}_{t \in T}
h_1(t), \dots \hbox{arg min}_{t \in T} h_k(t)]$ (which is called the
signature of the set). Less formally, the minhash at position i is the
minimal value of the hash function for any element of the set.

The Jaccard distance of two sets can be estimated by calculating the
number of positions on which the minhash signatures $s_1$, $s_2$ of the
two sets agree:

$$\hbox{EstimateJD}(s_1,s_2)=1-\frac{|\{i|i\in 1..k, s_1(i)=s_2(i)\}|}{|s_1|}$$

\textit{Locality-sensitive hashing} is a further technique to make
runtimes shorter. Let $S=\{s_1, \dots s_n\}$ be a set of minhash
signatures of all tests. Now a matrix $M \in \mathbb{R}^{n \times k}$
is created, by using the minhash signatures as columns. The rows of this
matrix are now divided into bands of length $r$. Example:

% CANDO: add matrix stuff, LSHs, lines between buckets, row indicators
% See sheet notes

$$
\matrix{
	s_1 & s_2 & s_3 & s_4 \cr
	9 & 4 & 1 & 1 \cr
	3 & 8 & 0 & 5 \cr
	0 & 2 & 4 & 5 \cr
	0 & 4 & 2 & 9 \cr
	2 & 7 & 6 & 3 \cr
	1 & 4 & 0 & 6 \cr
}
$$

When the hash of the values in different columns of two bands is the
same (the two minhashes are in the same bucket), the two test cases the
minhashes belong to are added to the confusingly named candidate set
(tests in the candidate set are excluded from selection). The candidate
set contains tests that have a Jaccard distance over a below threshold s,
where $s \approx 1-(r/n)^{1/r}$.

The algorithm follows different steps:

1. Calculate the minhashes \\
2. Calculate the candidate set $C_s$ \\
3. Select a test from the set of remaining tests $T \backslash C_s$ \\
4. Remove that test from the remaining tests $T$ and add it to the set of selected tests $S$ \\
5. If the condition is not fulfilled (budget not fulfilled in the inadequate case, coverage not achieved in the adequate case), go to step 2 \\
6. Return $S$ \\

%CANDO: formulate less confusingly?

The function that selects the test case from the remaining test cases
maximizes the Jaccard distance of the returned test and the so far
selected test cases $S$ (where $M$ is a function that produces minhashes, and Estimate:

$$ \hbox{arg max}_{c \in T \backslash C_s} \hbox{EstimateJD}(M(S), M(c))$$

\subsubsection{FAST-all}

FAST-all is very similar to FAST-pw. The only difference lies in the
selection function: While FAST-pw chooses based on Jaccard distance,
FAST-all selects all of $T \backslash C_s$, simply choosing all
non-candidate tests. This is a special case for the originally described
algorithm, which allowed to select a random sample from the non-candidate
tests with a given size ($one$, $log$, $sqrt$, $all$).

\subsection{Random Selection}

$\frac{1}{2}$ a page

\cite{khan2018systematic} gives 12 recommendations on how to execute and
evaluate test suite reduction experiments. \cite{cruciani2019scalable}
follows 11 out of these 12 analyses, but does not include a simple
alternative baseline, such as random selection from the set of tests.

This analysis adds an algorithm that randomly selects tests from the test
set.
In the inadequate case, it simply chooses $B$ random tests from the
test set:

%TODO: perhaps make these just pseudocode? Or leave out completely?

\begin{figure}[htpb]
\centering
\begin{tabular}{c}
\begin{lstlisting}[language=python]
def random_selection(input_file, B=0):
	TS = loadTestSuite(input_file)
	sel=random.sample(list(range(1,len(TS)+1)), min(B, len(TS)))
	return sel
\end{lstlisting}
\end{tabular}
\caption[Random budget selection]{The algorithm for selecting $B$ tests from the test suite randomly}\label{fig:random-budget-listing}
\end{figure}

In the adequate case, random selection removes a random test from the set
of tests and adds it to the selected tests until the coverage is adequate.

\begin{figure}[htpb]
\centering
\begin{tabular}{c}
\begin{lstlisting}[language=python]
def random_selection_adequate(input_file, B=0): 
	TS = loadTestSuite(input_file) 

	selcov=set() 
	allcov=set() 
	for k in list(TS.keys()): 
		allcov=allcov.union(TS[k]) 

	tests=set(range(1, len(TS)+1)) 
	sel=list() 

	while len(tests)>0 and len(allcov.difference(selcov))>0:
		selected=random.sample(tests, 1)[0] 
		sel.append(selected) 
		tests.remove(selected) 
		selcov=selcov.union(TS[selected]) 

	return sel
\end{lstlisting}
\end{tabular}
\caption[Random adequate selection]{The algorithm for selecting tests from the test suite randomly, adequate case}\label{fig:random-adequate-listing}
\end{figure}

\subsection{Adaptive Random Testing}

2 pages

\subsubsection{ART-D}

\subsubsection{ART-F}

\subsection{Greedy Algorithm}

$\frac{1}{2}$ a page
