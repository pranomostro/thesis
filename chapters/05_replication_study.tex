% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Replication Study}\label{chapter:replication_study}

%CANDO: where to put the information about bugs discovered in the original code?

17 pages

\section{Research Questions}

We define five %CANDO: change the number if we add/drop research questions
research questions, adapting three from \cite{cruciani2019scalable}
and adding two further questions.

\subparagraph{Research Question 1.1: Can the relative effectiveness in TSR be replicated on other study objects?}

We care about the size of the reduced test suite, therefore it is valuable
by how much test suites are reduced. We use the TSR metric (defined
in terms and definitions) to evaluate the magnitude of the reduction.
Using the TSR is mostly useful in adequate scenarios, since in inadequate
scenarios, the size of the reduced test suite relative to the original
test suite is fixed. We rank the algorithms using a Kruskal-Wallis test,
taken from \cite{cruciani2019scalable}

\subparagraph{Research Question 1.2: Can the relative effectiveness in FDL be replicated on other study objects?}

\cite{rothermel2002empirical} finds that test suite reduction can severely
compromise fault detection capability. To test whether this is the case,
the FDL metric is used to examine the fault detection capabilities
for the algorithms given new data. We also rank the methods using a
Kruskal-Wallis test.

\paragraph{Research Question 2: Can the relative runtime performance of the different algorithms be replicated?}

Due to using different datasets (of both larger and smaller size), the
absolute runtimes of the different test suite reduction methods can be
expected to be different from the original runs. However, the ordering of
the methods according to runtime is expected to be the same (that is, if
algorithm $X$ was faster than algorithm $Y$ on the old data, we can expect
algorithm $X$ to also be faster than algorithm $Y$ on the new data).

Again, we use the Kruskal-Wallis test to rank methods after their
runtime performance.

\paragraph{Research Question 3: How much better than random selection are specialized algorithms?}

\cite{khan2018systematic} recommends a comparison of the examined
technique to a simple baseline. We use their recommendation (random
selection) and compare the other approaches to how well they fare against
random selection on runtime, TSR and FDL.

\paragraph{(Possibly) Research Question 4: Can the results be reproduced with the original test data?}

\section{Study Design}

We approached the research questions by collecting fault data, coverage
data and test case source code from projects that had not been used in
\cite{cruciani2019scalable}, and running the code from that paper on the
new data. We then generated descriptive statistics (the mean, median,
and standard deviation) on the performance (i.e. FDL, TSR, and runtime)
of the different algorithms. Furthermore, we ordered the methods by
median, and performed a Kruskal-Wallis test to create a statistically
significant ordering of the different methods on the metrics evaluated.

\section{Study Objects}

The code bases and test suites selected for this study were small to
medium sized open-source Java projects. They all used either JUnit
4 or JUnit 5 as their testing framework, and the tool Maven %CANDO: right capitalization?
for building and testing. We used the latest version.

We selected java projects because tools for generating coverage
information and mutation-test data using Maven are available and
comparatively easy to use. We decided to use open-source projects to make
checking our generated data easier, as well as for availability reasons.

The selected projects are both under development and in use, which makes
our results relevant to real-life systems.

\section{Selecting Projects}

The projects used, their versions and size are presented in table CANDO
(project and test suite size are in lines of code, the version is the
6-digit prefix of the git commit used).

\begin{table}[htpb]
	\caption[]{}\label{tab:projects} %Project name, version, size of test suite, size of project, link to project
	\centering
	\begin{tabular}{l l l l l l}
		\toprule
		Project name & Version & Project size & Test-suite size & Number of tests \\
		\midrule
		assertj-core & cb2829 & 135k & 241k &3578 \\
		commons-collections & 242918 & 59k & 45k & 238 \\
		commons-lang & 6b3f25 & 51k & 40k & 181 \\
		commons-math & 649b13 & 148k & 98k & 438 \\
		jopt-simple & 5a1d72 & 1.7k & 2.7k & 145 \\
		jsoup & 89580c & 18k & 12k & 52 \\
		\bottomrule
	\end{tabular}
\end{table}

%CANDO: include number of test cases?

\section{Study Setup}

Testing the performance of the algorithms required three different kinds
of information: the contents of the test suites, coverage information
and fault information.

\subsection{Combining Tests Suites}

For every examined project, the test suite was converted into a format
suitable for the code from \cite{cruciani2019scalable} by replacing the
newlines from each test with spaces and concatenating the tests into
one file (the \textbf{black-box file}), such that every line contained
one test case.

For some of the projects, test cases were excluded since they failed
during the default test run:

%CANDO: make second column multi-line

\begin{table}[htpb]
	\caption[]{}\label{tab:excluded} %Project name, excluded tests
	\centering
	\begin{tabular}{l | p{10cm}}
		\toprule
		Project name & Test classes excluded \\
		\midrule
		assertj-core & BDDSoftAssertionsTest, SoftAssertionsTest, SoftAssertions\_overriding\_afterAssertionErrorCollected\_Test, SoftAssertionsErrorsCollectedTest, SoftAssertionsMultipleProjectsTest, SoftAssertions\_setAfterAssertionErrorCollected\_Test, AssertJMultipleFailuresError\_getMessage\_Test \\
		commons-collections & BulkTest \\
		commons-lang & FieldUtilsTest \\
		commons-math & FastMathTest, EvaluationTestValidation \\
		jopt-simple & / \\
		jsoup & / \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Generating Coverage Information}

Coverage information was generated using jacoco with the testwise mode
of the teamscale jacoco agent. %CANDO link

Since the teamscale jacoco agent only supports line coverage, other
types of coverage had to be eschewed.

%CANDO: add note about how line coverage subsumes branch & function coverage?

The json files created by the teamscale jacoco agent were converted
from JSON into the format used by \cite{cruciani2019scalable}: a file
containing a list of numbers in each line, with the numbers $n_1, \dots
n_i$ in line $n$ corresponding to the source lines of code in the tested
project covered by the test case at line $n$ in the \textbf{black-box
file}.

\subsection{Collecting Fault Coverage Information}

Fault detection information was not available for the projects used,
and was therefore generated by mutation testing using the mutation
testing framework pitest (first described in \cite{budd1980mutation}.

The generated mutation test data was converted from XML to the format used
in the code of \cite{cruciani2019scalable}: a text file containing a list
of number per line, the numbers at line $n$ corresponding to different
classes for which the test case at line $n$ in the \textbf{black-box file}
did find faults.

\section{Results}

To obtain the results, we used the code from \cite{cruciani2019scalable},
and applied the test suite reduction algorithms to new test data
(i.e. black-box test suite files and coverage information of 6 open source
projects) after minimal modification. These modifications were as follows:

We added a further algorithm for the random selection of test cases.

We fixed three bugs we found in the code: an off-by-one error in loading
coverage information, a bug where in small test suites sometimes the
candidate set would be empty, and a mistake where a a measured variable
was not reported correctly for FAST-all. The changes made to the source
code can be found at CANDO.

In the budget scenario, we considered budgets between 1\% and 30\%, with
a step increase of 1\%. We considered only line coverage (functionally
equivalent to statement coverage), and performed 50 measurements of
FDL, TSR, preparation and reduction time.

In the adequate scenario, we also considered only line coverage, and
made 50 measurements of preparation time, coverage preparation time,
and reduction time.

We did not replicate the large-scale scenario.

All measurements were performed under a Ubuntu 20.04 64-bit system,
using 6 AMDÂ® Ryzen 5 4500u with radeon graphics processors, and 7.2
GiB of RAM available.

\subsection{Research Questions}

\paragraph{Research Question 1.1 (Can TSR findings be replicated?)}

\autoref{tab:tsr_stats} shows the descriptive statistical results
(median, standard deviation and ordering for the Kruskal-Wallis test)
in the adequate case.

For Java programs with statement coverage in the adequate case,
\cite{cruciani2019scalable} order the different methods of test suite
reduction according to TSR. Their results mostly agree with ours: They
rank GA highest, then FAST++ and FAST-pw, then FAST-CS, FAST-all below
that, and the ART family lowest; we also rank GA highest, followed by
FAST++, then FAST-CS and FAST-pw at the same level, then FAST-all, and
the ART family last (on par with random selection). However, the median
test suite reduction they report for these methods is much lower than what
we observe (at least for the java programs): even GA only achives a TSR
of 12.30, while in our experiment GA achieves a TSR of over 40. However,
their results with C programs are more stark than ours, achieving a TSR
of more than 90 for GA or FAST++.

%CANDO: Use Borda-count to determine the amount of agreement in these
%rankings?

Our data shows higher variance in TSR performance on all algorithms.

\begin{table}[htpb]
	\caption[TSR statistical results, adequate]{Different variables relating to TSR (mdn is the median, $\sigma$ being the standard deviation, and $\delta$ being the ranking in the Kruskal-Wallis test)}\label{tab:tsr_stats}
	\centering
	\begin{tabular}{| l | l | l | l | l |}
	\midrule
	Approach & $\mu$ & mdn & $\sigma$ & $\delta$ \\
	\midrule
	ART-D & 18.421 & 17.351 & 8.997 & (e) \\
	ART-F & 0.945 & 0.228 & 1.654 & (f) \\
	FAST++ & 36.116 & 33.811 & 13.581 & (b) \\
	FAST-all & 21.414 & 18.264 & 8.536 & (d) \\
	FAST-CS & 33.683 & 31.893 & 12.064 & (c) \\
	FAST-pw & 33.258 & 32.718 & 11.458 & (c) \\
	GA & 40.599 & 39.124 & 14.922 & (a) \\
	RS & 0.947 & 0.420 & 1.491 & (f) \\
	\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[h]
\caption[TSR boxplots, adequate]{Boxplots for TSR for different algorithms in the adequate case}\label{fig:tsr_box}
\centering
\includegraphics[scale=0.7]{figures/tsrs.png}
\end{figure}

\paragraph{Research Question 1.2}

\autoref{tab:fdl_stats} shows the descriptive statistics (mean, median,
standard deviation, and ordering according to the Kruskal-Wallis test)
for the FDL of different algorithms in both the budget and adequate case.

We compare our findings to FDL performance on statement-covered C
program data, since the fault information for the java programs used
in \cite{cruciani2019scalable} is very sparse (1-3 faults per program),
while fault information is much richer for the C programs.

In the budget scenario, we could not detect strong differences in
performance in FDL. All algorithms score equally, with the exception
of GA dominating every other algorithm and RS being dominated by every
other algorithm. Here, again, \cite{cruciani2019scalable} come to very
similar conclusions: They find that GA is statistically significantly
better than all other algorithms, which are in turn approximately equally
effective at detecting faults.

Here, we also find that the standard deviations in FDL for our data
is higher than the ones in \cite{cruciani2019scalable}. Similar to
their findings, we see no strong difference in variance for different
algorithms.

In the adequate case, our data ranks the algorithms into just two
categories: The better-performing category contains algorithms
from the ART family, FAST++ and FAST-CS, while the rest of
the approaches is sorted into the bucket of worse-performing
approaches. \cite{cruciani2019scalable} agrees that the ART family
is best-performing, but find that FAST-all is second-best, GA being
third-best, and FAST-CS, FAST-pw and FAST++ coming last. At the moment,
we don't have a hypothesis about why this might be the case. It seems
relevant, however, that the median fault detection loss for data was 0 for
all algorithms (the mean fault detection loss being non-zero, however),
just as the ART family and FAST-all in \cite{cruciani2019scalable}.
Perhaps this is caused by single tests both having large coverage and
detecting a majority of faults.

This hypothesis is further supported by the observation that the variance
in performance on all algorithms is much lower than in the original paper,
for all algorithms.

\begin{table}[htpb]
	\caption[FDL statistical results]{Different variables relating to FDL ($\mu$ being the mean, mdn the median, $\sigma$ the standard deviation, and $\delta$ the ranking in the Kruskal-Wallis test)}\label{tab:fdl_stats}
	\centering
	\begin{tabular}{| c | c | c}
	\midrule
	Budget & Adequate \\
	\midrule
	{\begin{tabular}{l | l | l | l | l}
		Approach & $\mu$ & mdn & $\sigma$ & $\delta$ \\
		\midrule
		ART-D & 76.414 & 94.75 & 34.502 & (b) \\
		ART-F & 77.72 & 95.04 & 35.684 & (b) \\
		FAST++ & 76.49 & 94.05 & 35.662 & (b) \\
		FAST-all & 74.78 & 92.19 & 35.269 & (b) \\
		FAST-CS & 76.19 & 93.39 & 35.493 & (b) \\
		FAST-pw & 71.63 & 93.82 & 35.665 & (b) \\
		GA & 62.53 & 70.45 & 33.367 & (a) \\
		RS & 93.43 & 97.46 & 9.305 & (c) \\
	\end{tabular}} &
	{ \begin{tabular}{l | l | l | l | l}
		Approach & $\mu$ & mdn & $\sigma$ & $\delta$ \\
		\midrule
		ART-D & 0.129 & 0.0 & 0.702 & (a) \\
		ART-F & 0.057 & 0.0 & 0.679 & (a) \\
		FAST++ & 1.122 & 0.0 & 2.172 & (a) \\
		FAST-all & 0.155 & 0.0 & 0.331 & (b) \\
		FAST-CS & 1.144 & 0.0 & 2.367 & (a) \\
		FAST-pw & 0.792 & 0.0 & 1.131 & (b) \\
		GA & 1.432 & 0.0 & 2.444 & (b) \\
		RS & 0.058 & 0.0 & 0.679 & (b) \\
	\end{tabular}} \\
	\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[h]
\caption[FDL boxplots]{Boxplots for FDL for different algorithms}\label{fig:fdl_box}
\centering
\includegraphics[scale=0.7]{figures/fdls.png}
\end{figure}

We include the mean in the descriptive statistics to illustrate that
while the median adequate reduction has no fault detection loss, it can
nonetheless still occur.

\paragraph{Research Question 2}

Research question 2 asks whether the runtime performance of different
algorithms compare to each other.

\paragraph{Research Question 3}

\paragraph{(Possibly) Research Question 4}

\section{Discussion}

Comparison to ''Scalable Approaches to Test Suite Reduction''

\section{Threats to Validity}

Although we attempted to prevent it, the results we obtained might suffer
from threats to validity.

\subsection{Construct Validity}

Construct validity considers whether the methods are adequate to answer
the research questions posed. As in the original paper, we used FDL
and TSR as standard metrics, and reported the mean, median and standard
deviation of the data as standard statistical properties of the generated
data. We used faults generated by mutation testing software instead
of real faults, which might be dissimilar from real faults.
%CANDO: add citation that argues that mutation faults are similar
%to real-world faults

\subsection{Internal Validity}

The results we obtained might not be due to actual differences in
the performance of the different methods, but due to other factors,
such as the usage of the computer used for gathering data for other
purposes during that time. To migitate these effects, we performed every
measurement 50 times, and abstained from using the computer during the
time of gathering data.

%CANDO: add point about very different runtimes on different
%projects, maybe attach distribution images?

\subsection{Conclusion Validity}

Threats to conclusion validity are present when false positives are
found due to considering many different test suite reduction methods
and variables. This paper performs 4 %CANDO: update
statistical tests, thus we judge a significance level of 5\% to be
sufficient.

\subsection{External Validity}

The projects we selected are all small to medium-sized projects,
and therefore the results we obtained might not generalize to
large-scale software systems. While one of the projects we obtained
test data from (assertj-core) was larger than the projects examined in
\cite{cruciani2019scalable}, it was not analyzed separately from the
other data. However, since the projects we used were of similar size
as the ones in \cite{cruciani2019scalable}, and our interest was to
replicate their findings, we regard this consideration as less relevant.
